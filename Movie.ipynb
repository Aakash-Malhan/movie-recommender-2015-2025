{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# === Install lightweight deps (few minutes on first run) ===\n",
        "!pip -q install pandas numpy scipy scikit-learn gradio\n",
        "\n",
        "# === Imports ===\n",
        "import re, zipfile, urllib.request, os, gc, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset loader: MovieLens 25M\n",
        "# (robust to nested folders, filters years 2015â€“2025)\n",
        "# -----------------------------\n",
        "CACHE_DIR = Path.home() / \".cache\" / \"recsys25m\"\n",
        "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "ML25M_URL = \"https://files.grouplens.org/datasets/movielens/ml-25m.zip\"\n",
        "YEAR_RE = re.compile(r\"\\((\\d{4})\\)$\")\n",
        "\n",
        "def _download_ml25m() -> Path:\n",
        "    zpath = CACHE_DIR / \"ml-25m.zip\"\n",
        "    if not zpath.exists():\n",
        "        print(\"Downloading MovieLens 25M (~250MB)â€¦\")\n",
        "        urllib.request.urlretrieve(ML25M_URL, zpath)\n",
        "    return zpath\n",
        "\n",
        "def _extract_root_containing_csvs() -> Path:\n",
        "    # Find a folder that has movies.csv and ratings.csv\n",
        "    candidates = []\n",
        "    for p in CACHE_DIR.rglob(\"movies.csv\"):\n",
        "        cand = p.parent\n",
        "        if (cand / \"ratings.csv\").exists():\n",
        "            candidates.append(cand)\n",
        "    if candidates:\n",
        "        return sorted(candidates, key=lambda p: len(str(p)))[0]\n",
        "\n",
        "    # Not extracted yet â†’ extract zip to CACHE_DIR\n",
        "    with zipfile.ZipFile(_download_ml25m(), \"r\") as z:\n",
        "        z.extractall(CACHE_DIR)\n",
        "    # Search again\n",
        "    for p in CACHE_DIR.rglob(\"movies.csv\"):\n",
        "        cand = p.parent\n",
        "        if (cand / \"ratings.csv\").exists():\n",
        "            candidates.append(cand)\n",
        "    if not candidates:\n",
        "        raise FileNotFoundError(\"Could not find movies.csv/ratings.csv after extraction.\")\n",
        "    return sorted(candidates, key=lambda p: len(str(p)))[0]\n",
        "\n",
        "def _year_from_title(title: str) -> Optional[int]:\n",
        "    m = YEAR_RE.search(title or \"\")\n",
        "    return int(m.group(1)) if m else None\n",
        "\n",
        "def load_ml25m_2015_2025(\n",
        "    year_min=2015, year_max=2025,\n",
        "    min_user_ratings=10,\n",
        "    max_users=50_000,              # cap most active users (speed)\n",
        "    fast_mode_max_ratings=2_000_000 # optional downsample (speed)\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    root = _extract_root_containing_csvs()\n",
        "\n",
        "    movies = pd.read_csv(root / \"movies.csv\", usecols=[\"movieId\",\"title\",\"genres\"])\n",
        "    movies[\"year\"] = movies[\"title\"].map(_year_from_title)\n",
        "    movies = movies.dropna(subset=[\"year\"]).astype({\"year\":\"int16\"})\n",
        "    movies = movies[(movies[\"year\"] >= year_min) & (movies[\"year\"] <= year_max)]\n",
        "    movies = movies.rename(columns={\"movieId\":\"item_id\"})\n",
        "\n",
        "    ratings = pd.read_csv(root / \"ratings.csv\",\n",
        "                          usecols=[\"userId\",\"movieId\",\"rating\",\"timestamp\"],\n",
        "                          dtype={\"userId\":\"int32\",\"movieId\":\"int32\",\"rating\":\"float32\",\"timestamp\":\"int64\"})\n",
        "    ratings = ratings.rename(columns={\"userId\":\"user_id\",\"movieId\":\"item_id\"})\n",
        "    ratings = ratings[ratings[\"item_id\"].isin(movies[\"item_id\"])]\n",
        "\n",
        "    # keep active users (after year filter)\n",
        "    user_counts = ratings[\"user_id\"].value_counts()\n",
        "    active = user_counts[user_counts >= min_user_ratings].index\n",
        "    ratings = ratings[ratings[\"user_id\"].isin(active)]\n",
        "\n",
        "    # cap top most-active users for speed (optional)\n",
        "    if max_users is not None and len(active) > max_users:\n",
        "        top_users = user_counts.loc[active].nlargest(max_users).index\n",
        "        ratings = ratings[ratings[\"user_id\"].isin(top_users)]\n",
        "\n",
        "    # optional random downsample for speed\n",
        "    if fast_mode_max_ratings is not None and len(ratings) > fast_mode_max_ratings:\n",
        "        ratings = ratings.sample(fast_mode_max_ratings, random_state=42)\n",
        "\n",
        "    # keep only movies that still have ratings\n",
        "    movies = movies[movies[\"item_id\"].isin(ratings[\"item_id\"].unique())]\n",
        "\n",
        "    print(f\"[ML-25M] After filters: ratings={len(ratings):,}, users={ratings.user_id.nunique():,}, movies={len(movies):,}\")\n",
        "    return ratings[[\"user_id\",\"item_id\",\"rating\",\"timestamp\"]], movies[[\"item_id\",\"title\",\"year\",\"genres\"]]\n",
        "\n",
        "# -----------------------------\n",
        "# Recommender: TruncatedSVD MF\n",
        "# -----------------------------\n",
        "class MFRecommender:\n",
        "    def __init__(self, n_components=100, random_state=42):\n",
        "        self.svd = TruncatedSVD(n_components=n_components, random_state=random_state)\n",
        "        self.global_mean = 0.0\n",
        "        self.user_index = {}\n",
        "        self.item_index = {}\n",
        "        self.index_user = {}\n",
        "        self.index_item = {}\n",
        "        self.U = None\n",
        "        self.V = None\n",
        "\n",
        "    def _build_maps(self, ratings: pd.DataFrame):\n",
        "        users = ratings[\"user_id\"].astype(int).unique()\n",
        "        items = ratings[\"item_id\"].astype(int).unique()\n",
        "        self.user_index = {u:i for i,u in enumerate(sorted(users))}\n",
        "        self.item_index = {m:i for i,m in enumerate(sorted(items))}\n",
        "        self.index_user = {i:u for u,i in self.user_index.items()}\n",
        "        self.index_item = {i:m for m,i in self.item_index.items()}\n",
        "\n",
        "    def _to_sparse(self, ratings: pd.DataFrame) -> csr_matrix:\n",
        "        rows = ratings[\"user_id\"].map(self.user_index).to_numpy()\n",
        "        cols = ratings[\"item_id\"].map(self.item_index).to_numpy()\n",
        "        data = ratings[\"rating\"].astype(float).to_numpy()\n",
        "        return csr_matrix((data, (rows, cols)), shape=(len(self.user_index), len(self.item_index)))\n",
        "\n",
        "    def fit(self, ratings: pd.DataFrame):\n",
        "        self._build_maps(ratings)\n",
        "        R = self._to_sparse(ratings).astype(np.float32)\n",
        "        self.global_mean = (R.data.mean() if R.nnz > 0 else 0.0)\n",
        "        R = R.copy()\n",
        "        if R.nnz > 0:\n",
        "            R.data = R.data - self.global_mean\n",
        "        self.svd.fit(R)\n",
        "        V = self.svd.components_.T   # items x k\n",
        "        U = R @ V                    # users x k\n",
        "        # normalize for stability\n",
        "        V = V / (np.linalg.norm(V, axis=1, keepdims=True) + 1e-8)\n",
        "        U = U / (np.linalg.norm(U, axis=1, keepdims=True) + 1e-8)\n",
        "        self.V, self.U = V, U\n",
        "        return self\n",
        "\n",
        "    def recommend_from_liked(self, liked_item_ids, topk=10, exclude_item_ids=None):\n",
        "        liked_idx = [self.item_index[i] for i in liked_item_ids if i in self.item_index]\n",
        "        if not liked_idx:\n",
        "            scores = np.zeros(len(self.index_item)) + self.global_mean\n",
        "            order = np.argsort(-scores)\n",
        "        else:\n",
        "            u_vec = self.V[liked_idx, :].mean(axis=0)\n",
        "            u_vec = u_vec / (np.linalg.norm(u_vec) + 1e-8)\n",
        "            scores = u_vec @ self.V.T + self.global_mean\n",
        "            order = np.argsort(-scores)\n",
        "        exclude = set([self.item_index[i] for i in (exclude_item_ids or []) if i in self.item_index])\n",
        "        order = [i for i in order if i not in exclude]\n",
        "        top = order[:topk]\n",
        "        return [ (self.index_item[i], float(scores[i])) for i in top ]\n",
        "\n",
        "# -----------------------------\n",
        "# Train on 2015â€“2025 subset\n",
        "# -----------------------------\n",
        "ratings, movies = load_ml25m_2015_2025(\n",
        "    year_min=2015, year_max=2025,\n",
        "    min_user_ratings=10,\n",
        "    max_users=50_000,              # you may lower to 30_000 if RAM is tight\n",
        "    fast_mode_max_ratings=2_000_000\n",
        ")\n",
        "\n",
        "model = MFRecommender(n_components=100, random_state=42).fit(ratings)\n",
        "\n",
        "# Helper: quick title lookup\n",
        "def find_titles(q, k=10):\n",
        "    q = q.strip().lower()\n",
        "    if not q: return []\n",
        "    return movies[movies[\"title\"].str.lower().str.contains(q)].head(k)[\"title\"].tolist()\n",
        "\n",
        "# Quick smoke test (change titles as you like)\n",
        "sample_titles = [\"Mad Max: Fury Road (2015)\", \"La La Land (2016)\", \"Spider-Man: Into the Spider-Verse (2018)\"]\n",
        "liked_ids = movies[movies[\"title\"].isin(sample_titles)][\"item_id\"].tolist()\n",
        "recs = model.recommend_from_liked(liked_ids, topk=10, exclude_item_ids=liked_ids)\n",
        "out = pd.DataFrame(recs, columns=[\"item_id\",\"score\"]).merge(movies, on=\"item_id\", how=\"left\")[[\"title\",\"year\",\"score\"]]\n",
        "print(\"\\nSample recommendations for:\", sample_titles, \"\\n\")\n",
        "print(out.to_string(index=False))\n",
        "\n",
        "gc.collect();\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgpghP2iIf0q",
        "outputId": "9fd9ff1d-32a0-4448-daa3-437301c96b23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading MovieLens 25M (~250MB)â€¦\n",
            "[ML-25M] After filters: ratings=736,601, users=16,939, movies=10,187\n",
            "\n",
            "Sample recommendations for: ['Mad Max: Fury Road (2015)', 'La La Land (2016)', 'Spider-Man: Into the Spider-Verse (2018)'] \n",
            "\n",
            "                                                   title  year    score\n",
            "                                        Excursion (2018)  2018 3.907298\n",
            "                                         Stickman (2017)  2017 3.889499\n",
            "                My Extraordinary Summer with Tess (2019)  2019 3.827049\n",
            "              Keith Richards: Under the Influence (2015)  2015 3.822817\n",
            "       Digimon Adventure Tri. - Chapter 6: Future (2018)  2018 3.818893\n",
            "Digimon Adventure Tri. - Chapter 2: Determination (2016)  2016 3.818892\n",
            "                              Birthday Wonderland (2019)  2019 3.818892\n",
            "  Digimon Adventure Tri. - Chapter 5: Coexistence (2017)  2017 3.818892\n",
            "              Lupin the Third: Lie of Fujiko Mine (2019)  2019 3.818887\n",
            "                              The Relative Worlds (2019)  2019 3.818887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "\n",
        "# Pick top-N most-rated modern movies as choices (keeps dropdown snappy)\n",
        "topN = 1000\n",
        "top_movies = (ratings.groupby(\"item_id\").size()\n",
        "              .sort_values(ascending=False).head(topN).index.tolist())\n",
        "choices = (movies[movies[\"item_id\"].isin(top_movies)]\n",
        "           .sort_values(\"title\")[\"title\"].tolist())\n",
        "\n",
        "def recommend_ui(selected_titles, topk):\n",
        "    if not selected_titles:\n",
        "        return pd.DataFrame(columns=[\"title\",\"year\",\"score\"])\n",
        "    liked_ids = movies[movies[\"title\"].isin(selected_titles)][\"item_id\"].tolist()\n",
        "    recs = model.recommend_from_liked(liked_ids, topk=topk, exclude_item_ids=liked_ids)\n",
        "    df = pd.DataFrame(recs, columns=[\"item_id\",\"score\"]).merge(\n",
        "        movies, on=\"item_id\", how=\"left\")[[\"title\",\"year\",\"score\"]]\n",
        "    return df\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=recommend_ui,\n",
        "    inputs=[\n",
        "        gr.Dropdown(choices=choices, label=\"Pick a few movies you like (2015â€“2025)\", multiselect=True),\n",
        "        gr.Slider(5, 30, value=10, step=1, label=\"Top-K\")\n",
        "    ],\n",
        "    outputs=gr.Dataframe(label=\"Recommendations\"),\n",
        "    title=\"ðŸŽ¬ Movie Recommender (2015â€“2025, MovieLens 25M)\",\n",
        "    description=\"Select a few modern movies; get similar recommendations. Model: MF with TruncatedSVD.\"\n",
        ")\n",
        "\n",
        "# share=True gives you a public URL; queue() handles concurrent clicks nicely\n",
        "demo.queue().launch(share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "3hl0Xn7BImE6",
        "outputId": "04765167-1a6c-4a24-bf4a-1d8755f8ee0c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://2ccf3e97c6feb71575.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2ccf3e97c6feb71575.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}